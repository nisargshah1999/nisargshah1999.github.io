<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Nisarg Shah | PhD Student & Researcher</title>

    <meta name="author" content="Nisarg Shah">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <meta name="description" content="Personal website of Nisarg A. Shah, a PhD student at Johns Hopkins University specializing in multi-modal foundation models, computer vision, and medical imaging.">
    <meta name="keywords" content="Nisarg Shah, Nisarg Anish Shah, JHU, Johns Hopkins, VIU Lab, Computer Vision, PhD Student, Multi-modal, Foundation Models, Video LLMs, Medical Imaging, Researcher">
    <link rel="canonical" href="https://www.nisargshah.com/" />

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/jpeg" href="images/jhu_symbol.jpg">

    <meta property="og:type" content="website">
    <meta property="og:url" content="https://www.nisargshah.com/">
    <meta property="og:title" content="Nisarg Anish Shah | PhD Student at Johns Hopkins University">
    <meta property="og:description" content="Personal website of Nisarg A. Shah, a PhD student at JHU working on multi-modal foundation models and computer vision.">
    <meta property="og:image" content="https://www.nisargshah.com/images/NS_PFP.png">
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://www.nisargshah.com/">
    <meta property="twitter:title" content="Nisarg Anish Shah | PhD Student at Johns Hopkins University">
    <meta property="twitter:description" content="Personal website of Nisarg A. Shah, a PhD student at JHU working on multi-modal foundation models and computer vision.">
    <meta property="twitter:image" content="https://www.nisargshah.com/images/NS_PFP.png">

    <style>
      /* This block applies dark mode automatically if the user's OS is set to dark */
      @media (prefers-color-scheme: dark) {
        body:not(.light-mode) {
          background-color: #121212;
          color: #e0e0e0;
        }
        body:not(.light-mode) a { color: #bb86fc; }
        body:not(.light-mode) papertitle { color: #e0e0e0; }
        body:not(.light-mode) tr[bgcolor="#ffffe6"] { background-color: #333322 !important; }
        body:not(.light-mode) .name { color: #ffffff; }
      }

      /* This block applies dark mode if manually toggled ON */
      body.dark-mode {
        background-color: #121212;
        color: #e0e0e0;
      }
      body.dark-mode a { color: #bb86fc; }
      body.dark-mode papertitle { color: #e0e0e0; }
      body.dark-mode tr[bgcolor="#ffffe6"] { background-color: #333322 !important; }
      body.dark-mode .name { color: #ffffff; }
      
      /* This block forces light mode if manually toggled ON */
      body.light-mode {
        background-color: #ffffff;
        color: #000000;
      }
      body.light-mode a { color: #1772d0; }
      body.light-mode papertitle { color: #000000; }
      body.light-mode tr[bgcolor="#ffffe6"] { background-color: #ffffe6 !important; }
      body.light-mode .name { color: #000000; }
    </style>

    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Person",
      "name": "Nisarg Anish Shah",
      "url": "https://www.nisargshah.com/",
      "image": "https://www.nisargshah.com/images/NS_PFP.png",
      "sameAs": [
        "https://scholar.google.com/citations?hl=en&user=uljVmswAAAAJ",
        "https://github.com/nisargshah1999",
        "https://twitter.com/nisarg0812",
        "https://www.linkedin.com/in/nisarg-shah-bbb0a0172/"
      ],
      "jobTitle": "PhD Student",
      "worksFor": {
        "@type": "Organization",
        "name": "Johns Hopkins University"
      },
      "alumniOf": {
        "@type": "CollegeOrUniversity",
        "name": "Indian Institute of Technology Jodhpur"
      },
      "knowsAbout": ["Computer Vision", "Multi-modal Foundation Models", "Medical Imaging", "Generative Models", "Video LLMs"]
    }
    </script>
    
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-29JZK4MXBB"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-29JZK4MXBB');
    </script>

  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
            <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr style="padding:0px">
                <td style="padding:0px">
                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr style="padding:0px">
                      <td style="padding:2.5%;width:63%;vertical-align:middle">
                        
                        <p class="name" style="text-align: center;">Nisarg Anish Shah</p>
                        <p>I am a fourth-year PhD student in the department of Electrical and Computer Engineering at <a href="https://www.jhu.edu/">Johns Hopkins University</a>, where I am a member of the <a href="https://engineering.jhu.edu/vpatel36/">VIU Lab</a> advised by <a href="https://scholar.google.com/citations?hl=en&user=AkEXTbIAAAAJ">Dr. Vishal Patel</a>.</p>
                        <p>During my PhD, I have been fortunate to have interned at AWS on Agentic AI with <a href="https://www.linkedin.com/in/pengkai-zhu-822345152/">Pengkai Zhu</a>, <a href="https://ankanbansal.com/">Ankan Bansal</a>, and <a href="https://scholar.google.com/citations?user=KQLmaxgAAAAJ&hl=en">Srikar Appalaraju</a>, as well as at Netflix Research, working with <a href="https://www.linkedin.com/in/amirziai/">Amir Ziai</a>, <a href="https://www.linkedin.com/in/chaitue">Chaitanya Ekanadham</a>, and <a href="https://www.linkedin.com/in/benjamin-klein-usa">Ben Klein</a> on Video LLMs.</p>
                        <p>Before my PhD, I was a Research Engineer at <a href="https://www.aifoundation.com/">AI Foundation</a>, where I worked with <a href="https://gauravbharaj.github.io/">Gaurav Bharaj</a> on efficient generative models. I completed my undergraduate studies in Electrical Engineering at the <a href="https://iitj.ac.in/">Indian Institute of Technology Jodhpur</a>, advised by <a href="https://www.iitj.ac.in/People/Profile/350ea9db-2521-402a-98e0-3a40a18f06a9">Dr. Anil Kumar Tiwari</a>.</p>

                        <p style="text-align:center">
                          <a href="mailto:snisarg812@gmail.com">Email</a> &nbsp;/&nbsp;
                          <a href="data/CV_Nisarg_F25.pdf">CV</a> &nbsp;/&nbsp;
                          <a href="https://scholar.google.com/citations?hl=en&user=uljVmswAAAAJ">Google Scholar</a> &nbsp;/&nbsp;
                          <a href="https://github.com/nisargshah1999">Github</a> &nbsp;/&nbsp;
                          <a href="https://twitter.com/nisarg0812">X (Twitter)</a> &nbsp;/&nbsp;
                          <a href="https://www.linkedin.com/in/nisarg-shah-bbb0a0172/">LinkedIn</a> &nbsp;/&nbsp;
                          <a href="#" onclick="toggleDarkMode()" style="text-decoration:none;">ðŸŒ™</a>
                        </p>
                      </td>
                      <td style="padding:2.5%;width:40%;max-width:40%">
                        <a href="images/NS_PFP.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/NS_PFP.png" class="hoverZoomLink"></a>
                      </td>
                    </tr>
                  </tbody></table>
                </td>
              </tr>
            </tbody></table>
            
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:20px;width:100%;vertical-align:middle">
                    <h2>Research Interest</h2>
                    <p>
                      My research lies at the intersection of computer vision and natural language processing, where I focus on building advance open-world and multi-modal foundation models. My long-term goal is to develop vision-based agents capable of advanced reasoning and planning, ultimately pushing the boundaries of what foundational models can achieve.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>News</h2>
              <ul>
                <li><strong>[July 2025]</strong> - Honored to receive a travel award for MIDL 2025!</li>
                <li><strong>[June 2025]</strong> - One paper accepted to MICCAI 2025.</li>
                <li><strong>[May 2025]</strong> - Started an exciting research internship at Amazon AWS focusing on Agentic AI!</li>
                <li><strong>[Jan 2025]</strong> - One paper accepted to MIDL 2025.</li>
                <li><strong>[June 2024]</strong> - Started a research internship at Netflix Research working on Video-LLMs!</li>
                <li><strong>[February 2024]</strong> - One paper accepted to CVPR 2024.</li>
                <li><strong>[June 2023]</strong> - One paper accepted to MICCAI 2023.</li>
              </ul>
            </td>
          </tr>
        </tbody></table>


<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Selected Publications</h2>
                  <p> Representative papers are <span style="background-color: #ffffe6">highlighted</span>. * denotes equal contribution.
                  </p>
                </td>
              </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          
          <tr bgcolor="#ffffe6">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cine.png" alt="CinÃ©aste" width="220">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2509.14227"><papertitle>CinÃ©aste: A Fine-grained Contextual Movie Question Answering Benchmark</papertitle></a>
                <br>
                <strong>Nisarg A Shah</strong>, Amir Ziai, Chaitanya Ekanadham, Vishal M Patel
                <br>
                <em>Under Review</em>, 2025
                <br>
                [<a href="https://arxiv.org/pdf/2509.14227">Paper</a>]
            </td>
          </tr>

          <tr bgcolor="#ffffe6">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/al.png" alt="StepAL" width="220">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2507.22059"><papertitle>StepAL: Step-aware Active Learning for Cataract Surgical Videos</papertitle></a>
                <br>
                <strong>Nisarg A Shah</strong>, Bardia Bonab, Shameema Sikder, S. Swaroop Vedula, Vishal M. Patel
                <br>
                <em>International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</em>, 2025
                <br>
                [<a href="https://arxiv.org/pdf/2507.22059">Paper</a>]
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/jepa.png" alt="A Vision Foundation Model for Cataract Surgery" width="220">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://openreview.net/pdf?id=QbBPeAIdrk"><papertitle>A Vision Foundation Model for Cataract Surgery Using Joint-Embedding Predictive Architecture</papertitle></a>
                <br>
                <strong>Nisarg A Shah</strong>, Mingze Xia, Subhasri Vijay, Shameema Sikder, S. Swaroop Vedula, Vishal M. Patel
                <br>
                <em>Medical Imaging with Deep Learning (MIDL)</em>, 2025
                <br>
                [<a href="https://openreview.net/pdf?id=QbBPeAIdrk">Paper</a>]
            </td>
          </tr>
          
          <tr bgcolor="#ffffe6">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/lqm.png" alt="LQMFormer" width="220">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Shah_LQMFormer_Language-aware_Query_Mask_Transformer_for_Referring_Image_Segmentation_CVPR_2024_paper.pdf"><papertitle>LQMFormer: Language-aware Query Mask Transformer for Referring Image Segmentation</papertitle></a>
                <br>
                <strong>Nisarg A Shah</strong>, Vibashan VS, Vishal M Patel
                <br>
                <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2024
                <br>
                [<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Shah_LQMFormer_Language-aware_Query_Mask_Transformer_for_Referring_Image_Segmentation_CVPR_2024_paper.pdf">Paper</a>]
                [<a href="#">Code</a>]
                [<a href="#">Project Page</a>]
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gls.png" alt="GLSFormer" width="220">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2307.11081"><papertitle>GLSFormer: Gated Long, Short Sequence Transformer for Step Recognition in Surgical Videos</papertitle></a>
                <br>
                <strong>Nisarg A Shah</strong>, Shameema Sikder, Swaroop Vedula, Vishal M Patel
                <br>
                <em>International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</em>, 2023
                <br>
                [<a href="https://arxiv.org/pdf/2307.11081">Paper</a>]
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/decig.png" alt="Device Efficient Conditional Image Generation" width="220">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2203.10363"><papertitle>Towards Device Efficient Conditional Image Generation</papertitle></a>
                <br>
                <strong>Nisarg A Shah</strong>, Gaurav Bharaj
                <br>
                <em>British Machine Vision Conference (BMVC)</em>, 2022
                <br>
                [<a href="https://arxiv.org/pdf/2203.10363">Paper</a>]
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/depth.png" alt="Deterministic Video Depth Forecasting" width="220">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2207.00506"><papertitle>How Far Can I Go? : A Self-Supervised Approach for Deterministic Video Depth Forecasting</papertitle></a>
                <br>
                Sauradip Nag*, <strong>Nisarg A Shah</strong>*, Anran Qi*, Raghavendra Ramachandra
                <br>
                <em>NeurIPS Workshop on Machine Learning for Autonomous Driving</em>, 2021
                <br>
                [<a href="https://arxiv.org/pdf/2207.00506">Paper</a>]
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/relight.png" alt="Efficient Deep Network for Image Relighting" width="220">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2102.09242"><papertitle>DSRN: an Efficient Deep Network for Image Relighting</papertitle></a>
                <br>
                Sourya Dipta Das*, <strong>Nisarg A Shah</strong>*, Saikat Dutta, Himanshu Kumar
                <br>
                <em>IEEE International Conference on Image Processing (ICIP)</em>, 2021
                <br>
                [<a href="https://arxiv.org/pdf/2102.09242">Paper</a>]
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/bokeh.png" alt="Bokeh Effect Rendering" width="220">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Dutta_Stacked_Deep_Multi-Scale_Hierarchical_Network_for_Fast_Bokeh_Effect_Rendering_CVPRW_2021_paper.pdf"><papertitle>Stacked Deep Multi-Scale Hierarchical Network for Fast Bokeh Effect Rendering</papertitle></a>
                <br>
                Saikat Dutta, Sourya Dipta Das, <strong>Nisarg A Shah</strong>
                <br>
                <em>CVPR Workshop on Mobile AI</em>, 2021
                <br>
                [<a href="https://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Dutta_Stacked_Deep_Multi-Scale_Hierarchical_Network_for_Fast_Bokeh_Effect_Rendering_CVPRW_2021_paper.pdf">Paper</a>]
            </td>
          </tr>
          
        </tbody></table>

     <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Academic Services</h2>
              <p>
                I frequently serve as a reviewer for conferences in Computer Vision and Medical Imaging. Recent venues include:
              </p>
              <ul>
                <li><strong>[August 2025]</strong> - Serving as a reviewer for WACV 2026</li>
                <li><strong>[May 2025]</strong> - Serving as a reviewer for ACMMM 2025</li>
                <li><strong>[March 2025]</strong> - Serving as a reviewer for MICCAI 2025</li>
                <li><strong>[December 2024]</strong> - Serving as a reviewer for CVPR 2025</li>
              </ul>
            </td>
          </tr>
        </tbody></table>

      </table>
    </tbody></table>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  Source code from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's website</a>. 
                </p>
            </td>
        </tr>
    </tbody></table>
        
        </td>
      </tr>
    </table>
    
    <!-- Dark Mode JavaScript (with system preference logic) -->
    <script>
      function toggleDarkMode() {
        // Check current state, including system preference
        const isCurrentlyDark = document.body.classList.contains('dark-mode') || 
                                (!localStorage.getItem('theme') && window.matchMedia('(prefers-color-scheme: dark)').matches);

        if (isCurrentlyDark) {
          // Switch to light mode
          document.body.classList.remove('dark-mode');
          document.body.classList.add('light-mode');
          localStorage.setItem('theme', 'light');
        } else {
          // Switch to dark mode
          document.body.classList.remove('light-mode');
          document.body.classList.add('dark-mode');
          localStorage.setItem('theme', 'dark');
        }
      }

      // Apply saved theme on page load, otherwise respect system settings via CSS
      (function () {
        const savedTheme = localStorage.getItem('theme');
        if (savedTheme === 'dark') {
          document.body.classList.add('dark-mode');
        } else if (savedTheme === 'light') {
          document.body.classList.add('light-mode');
        }
      })();
    </script>
    
  </body>
</html>
